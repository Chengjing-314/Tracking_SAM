{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aa12104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n",
      "/data/chengjingyuan/miniconda3/envs/tracking_SAM/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb696373-fae1-45a4-a05e-c93d4e8cae6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n",
      "/data/chengjingyuan/miniconda3/envs/tracking_SAM/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import groundingdino.datasets.transforms as T\n",
    "from groundingdino.models import build_model\n",
    "from groundingdino.util import box_ops\n",
    "from groundingdino.util.slconfig import SLConfig\n",
    "from groundingdino.util.utils import clean_state_dict\n",
    "from groundingdino.util.inference import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e2cea10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/chengjingyuan/miniconda3/envs/tracking_SAM/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n"
     ]
    }
   ],
   "source": [
    "config_file_path = './tracking_SAM/third_party/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py'\n",
    "model_path = './pretrained_weights/groundingdino_swint_ogc.pth'\n",
    "\n",
    "args = SLConfig.fromfile(config_file_path) \n",
    "device = 'cpu'\n",
    "\n",
    "dino_model = build_model(args)\n",
    "\n",
    "checkpoint = torch.load(model_path, map_location='cpu')\n",
    "log = dino_model.load_state_dict(clean_state_dict(checkpoint['model']), strict=False)\n",
    "dino_model.eval()\n",
    "dino_model = dino_model.to(device)\n",
    "\n",
    "print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a15e128",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_path = \"./sample_data/DAVIS_bear/images/00000.jpg\"\n",
    "image_np = np.asarray(Image.open(test_img_path).convert(\"RGB\"))\n",
    "\n",
    "transform = T.Compose(\n",
    "    [\n",
    "        T.RandomResize([800], max_size=1333),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "img_chw, _ = transform(Image.fromarray(image_np), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af08a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prompt = 'bear'\n",
    "\n",
    "BOX_TRESHOLD = 0.3\n",
    "TEXT_TRESHOLD = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eabf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes, logits, phrases = predict(\n",
    "    model=dino_model, \n",
    "    image=img_chw, \n",
    "    caption=text_prompt, \n",
    "    box_threshold=BOX_TRESHOLD, \n",
    "    text_threshold=TEXT_TRESHOLD,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4efed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W, _ = image_np.shape\n",
    "\n",
    "boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dbd1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_img = image_np.copy()\n",
    "\n",
    "for box, phrase in zip(boxes_xyxy, phrases):\n",
    "    box = box.cpu().numpy().astype(np.int32)\n",
    "    viz_img = cv2.rectangle(viz_img, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "    viz_img = cv2.putText(viz_img, phrase, (box[0], box[1]), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "plt.imshow(viz_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb99a52-2643-4c4a-8d6c-95d65e34f71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "sam_checkpoint = \"./pretrained_weights/sam_vit_h_4b8939.pth\"  # default model\n",
    "\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2f7226-f4b0-4bf1-bf33-957c082d07ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.set_image(image_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93114d2-f464-4fcb-8ff5-b658e4a3271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(boxes_xyxy) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5092a9de-5b8b-489b-b828-0b38d43ed21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_box = boxes_xyxy[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1536b2b6-ab79-43d3-a33b-705472afafe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks, _, _ = predictor.predict(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    box=input_box[None, :],\n",
    "    multimask_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21e1542-cfe8-4318-b0f5-1f5d6886c4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca7b69a-6bec-4b55-9e75-4908b18429bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tracking_SAM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
